교재에서 설명을 쉽게 해주셔서, ARIMA가 쉬운건가부다,,, 하다가
연습문제 풀면서 아주 생 고생을 했습니다.
정말 쉬운게 하나도 없네요.

겨우 시계열 부문 마무리하고, 산사랑님이 공유해주신 일정을 조금 변경하여 저의 취약점인
SNA를 먼저 할려고 합니다. 관광분석대회 수상작품중에 SNA를 중심으로한 팀에서 보신 참고자료중에
"R을 활용한 사회네트워크 분석입문"이 있어서, 찾아보니
얇고 연습문제도 적어서(?), 이를 가지고 진행할려고 합니다. ㅠㅠ;;


FPP : ARIMA Exercise 8.11
========================================================

1. Figure 8.24 shows the ACFs for 36 random numbers, 360 random numbers and for 1,000 random numbers.

- Explain the differences among these figures. Do they all indicate the data are white noise?
```{r 1_1}
# 데이터의 크기가 커질수록 표본분산이 낮아지는 것과 유사한 논리로 ACF의 critical value도 sqrt(T)로 나눠지는 만큼 작아지는 것이라고 이해할 수 있을 것 같다.

# 정규분포 가정시.
(first <- 1.96/sqrt(36)) # 0.33
(second <- 1.96/sqrt(360)) # 0.10
(third <- 1.96/sqrt(1000)) # 0.06
```

- Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?
```{r 1_2}
#Autocorrelation 개념자체가 이전 데이터와 현재 데이터간의 차이를 기반으로 하고 있기 때문에 잔차의 개념이라고 볼 수 있다고 생각되며, 통계적인 추론이 가능한 correlation이 없고, 에러가 특정한 확률분포를 따른다고 할 수 있는 데이터의 경우에 잔차의 평균은 0가된다. 

# 수학적으로는 white noise가 mean 0, Sd 1을 따르는 분포를 가진다고 가정을 하고 있기 때문이라는 생각이 든다.
```

2. A classic example of a non-stationary series is the daily closing IBM stock prices (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows the series is non-stationary and should be differenced.
```{r 2_ibmclose}
library(fpp)
#IBM 주식 1년간의 종가 데이터
plot(ibmclose)
tsdisplay(ibmclose) # tsdisplay 편하다.
# 그래프 상에서 ACF가 급속히 줄어들지 않고 있으나, PaCF는 lag1을 제외하고는 critical value범위안에 들어가고 있다. 따라서 seasoanl한 부분은 없다고 생각된다. 아마도 이는 first differencing이 필요하다는 강력한 증거라고 생각된다.

# differencing 필요여부 테스트
kpss.test(ibmclose) # 귀무가설 stationary 기각. (adf.test와는 반대의 귀무가설을 가지고 있다.)
adf.test(ibmclose) # 귀무가설 non stationary 기각 실패. non-stationalry.

plot(diff(ibmclose)) # whitenoise와 비슷한 모양이 나온다.
Box.test(diff(ibmclose), type="Lj") #하지만 미세하게 오토코릴레이션이 남아 있음을 알수 있다.
tsdisplay(diff(ibmclose)) #아마도 주간격으로 시즈널리티가 있는 모양이다. 교재에서 일단 시즈널리티를 제외한후 first difference를 하라는 말이 있었다.

plot(diff(ibmclose,7))
Box.test(diff(ibmclose,7), type="Lj") # 그래프 상으로는 first differencing보다 안좋은 것 같은데, 검증결과는 귀무가설기각, 즉 stationary한 것으로 나온다. 귀무가설이 자꾸 헷갈린다. 귀무가설은 not stationary하다는 것이다. 
```

3. For the following series, find an appropriate Box-Cox transformation and order of differencing in order to obtain stationary data.

- usnetelec
```{r 3_1}
usnetelec #연간데이터이다.
tsdisplay(usnetelec) # 모양이 그냥 firstdifferencing이면 될 것 같은 기분이 든다.
plot(diff(usnetelec))
Box.test(diff(usnetelec)) # 근데아니다. ㅠㅠ

(lambda <- BoxCox.lambda(usnetelec))
plot(BoxCox(usnetelec, lambda))
tsdisplay(BoxCox(usnetelec, lambda))
(usnetBC <- BoxCox(usnetelec, lambda))
plot(diff(usnetelec))
Box.test(usnetBC) # 통과다. 육안으로는 거의 분산의 변화가 안보이는 데, 데이터 상으로는 그런 모양이다.

#추가 : 다시 생각해보니, 이 문제는 모델이 완성된후에 잔차의 white noise 여부를 검증하는 것이 아니기 때문에 Box.test는 잘못된 방법인 것 같다.
# kpss테스트나 ndiffs(first differencing)나 nsdiffs가 맞는 방법인것 같다.
(nd <- ndiffs(usnetBC))
(nd <- nsdiffs(usnetBC)) # seasonal이 없는데 집어넣으면 에러가 남.
#
#
adf.test(usnetBC, alternative = "stationary") # 귀무가설 기각 실패
kpss.test(usnetBC) #귀무가설이 스태이셔너리 하다는 것이다.
#따라서 p-value가 낮으면 기각되어 non-stationary가 된다.
?kpss.test
?adf.test #unit-root 같은 뿌리에서 나왔다는 의미에서 stationary test의 의미인 모양이다.

# ndiffs가 2이므로, 2번 differencing할려면...
usnetSt <- diff(usnetBC, differences=nd)
adf.test(usnetSt) # 귀무가설 기각 따라서 stationary
kpss.test(usnetSt) # 귀무가설 기각 실패. 해석에 주의 필요. 역시 통과.
tsdisplay(usnetSt) 
Box.test(usnetSt) # 통과다.
```

usgdp
```{r 3_2}
usgdp # 분기별 데이터이다.
tsdisplay(usgdp) #1번그림과는 큰 차이가 난다. ACF가 산술적으로 감소한다. 이건 진짜 first differencing만 하며 될 것 같다.
plot(diff(usgdp)) # 불길하게 약간 우상향의 트렌드가 있는 것 같다.
Box.test(diff(usgdp)) # 하지만 일단 통과다. 다행히도 운이 좋데 시즈널리티가 없는 데이터여서 그런 것 같다.

# 추가
nsdiffs(usgdp)
ndiffs(usgdp)
adf.test(usgdp)
usgdpSt <- diff(usgdp, differences=2)
adf.test(usgdpSt)
kpss.test(usgdpSt) # 이경우에는 기각이 아니다. kpss테스트가 더 엄격한 조건을 부여하는 것을 알수 있다.
Box.test(usgdpSt)
adf.test(diff(usgdp)) # first differencing을 한번만 해도 통과는 된다. ndiffs는 조금 보수적으로 보아서 확실하게 답을 내는 모양이다.
kpss.test(diff(usgdp)) # kpss는 기각으로 나온다.
```

mcopper
```{r 3_3}
unclass(mcopper) #월별 데이터이다. ACF가 기하급수와 비슷하게 감소하고, PACF도 3,6,9,12근처에서 critical value에 근접하거나 넘는다. 분기별 시즈널리티가 있는 모양이다.
tsdisplay(mcopper)
plot(diff(mcopper,4))
Box.test(diff(mcopper,4)) #통과다. ㅋㅋㅋ.

# 추가
nsdiffs(mcopper) #이상하다. 0이 나온다.
ndiffs(mcopper)
adf.test(mcopper) # 귀무가설 기각 실패. stationary하지 않다.
kpss.test(mcopper) # 귀무가설 기각. stationary하지 않다.
#일단 ndiffs의 결과를 따라보면,
mcopperSt <- diff(mcopper)
tsdisplay(mcopperSt)
adf.test(mcopperSt) # 귀무가설 기각 stationary.
kpss.test(mcopperSt) # 귀무가설 유지 stationary.
```

enplanements
```{r 3_4}
unclass(enplanements) #월별 데이터이다.
tsdisplay(enplanements) #ACF도 들쭉날쭉하고 마찬가지로 PaCF도 그렇다. 진폭도 갈수록 커져서 분산의 안정화도 필요한 것같다.
lambda <- BoxCox.lambda(enplanements)
enplaneBC <- BoxCox(enplanements, lambda)
tsdisplay(enplaneBC) # 상단그래프의 진폭은 대충 맞은 것 같은데, 아래는 그대로다.
# 교재에서 가르쳐준대로 시즈널리티를 먼저 디퍼런싱한다.
# 거의 매달 Pacf가 크리티컬 밸류를 넘으므로 m=12로 해본다.
tsdisplay(diff(enplaneBC,12)) # 일단 많이 줄어들었는데, 여전히 모양이 불규칙하다.
tsdisplay(diff(enplaneBC,4)) # 모양이 더안좋아진다, 12가 맞는것 같다.
# stationary하게 하기위한 first differencing을 추가 한다.
tsdisplay(diff(diff(enplaneBC,12))) #lag1과 lag12가 반복적으로 ACF와 PACF에서 크리티컬 밸류를 넘는다.
tsdisplay(diff(diff(diff(enplaneBC,12)))) # 아이고, 더이상해진다. 쉽지 않다.
Box.test(diff(diff(enplaneBC,12))) # 일단 시즌1번, first 1번의 디퍼런싱으로 통과할 수 있는 모양이 나온다.

# 호기심에 과한 디퍼런싱의 결과는?
Box.test(diff(diff(diff(enplaneBC,12)))) #이것도 white noise다. 화이트 노이즈의 디퍼런스는 당연히 화이트노이즈인가보다.

# 추가
nsdiffs(enplanements) #1번 필요
ndiffs(enplanements) # 1번 필요
# 시즌널 디퍼런싱 우선
enplaneSt <- diff(enplanements,12)
kpss.test(enplaneSt) # 시즌 디퍼런싱 만으로도 일단 개선은 됨.
enplaneSt <- diff(enplaneSt)
kpss.test(enplaneSt)

# 분산 안정화 전후 다시 점검 : 큰 차이는 없는 것으로보임.
lambda <- BoxCox.lambda(enplanements)
enplaneBC <- BoxCox(enplanements, lambda)
nsdiffs(enplaneBC)
ndiffs(enplaneBC)
enplaneSt <- diff(diff(enplanements,12))
adf.test(enplaneSt)
```

visitors
```{r 3_5}
unclass(visitors) 역시 월별데이터이다.
tsdisplay(visitors) # 분산도 커지는 것 같고, 시즈널리티와 fisrt differencing이 필요해 보인다.
lambda <- BoxCox.lambda(visitors)
visitBC <- BoxCox(visitors, lambda)
tsdisplay(visitBC)
tsdisplay(diff(visitBC,12))
tsdisplay(diff(diff(visitBC,12)))
adf.test(diff(diff(visitBC,12))) # 화이트 노이즈이긴한데, ACF와 PACF가 남아있어서 autocorrelation이 있어서 ARIMA나 지수평활법 등을 활용해 이들을 반영하면 될 것이다.
```

4. For the enplanements data, write down the differences you chose above using backshift operator notation.
[backshift notation](https://www.otexts.org/fpp/8/2) 참조

```{r 4_1}
# 이건 공식을 기술하라는 것인데, Backshift가 뭔지는 알겠는데, LaTex 실력이 모잘라서 공식을 손으로 적을 수 밖에 없지만, 일단 대충 적어본다.
unclass(enplanements) # 월별 데이터
tsdisplay(enplaneSt) # first differencing 1회, seasonal differencing 1회

# ACF가 첫번째만 lag1과 lag12가 critical value를 넘음. 그리고 PACF도 1과 12가 넘음. 따라서 AR(1)과 MA(1)을 합치는 것이 좋을 것으로 보임. 따라서 모델은 ARIMA(1,1,1) (1,1,1){12}로 예상

# backshift notation : {아래 첨자,subscript}, [윗첨자,superscript],
# ordinary는 소문자, seasonal은 대문자.

(1-phi{1}*B)*(1-PHI{1}*B[12])*(1-B)*(1-B[12])*y{t} = (1+theta{1}*B)*(1+THETA{1}*B[12])*e{t}

# 검산
auto.arima(enplaneSt) # ARIMA(1,0,1)(0,0,1) 아마도 diff처리를 했기때문인 것으로 보임.
auto.arima(enplanements) # seasonal differencing 1번만 하는 것으로나온다. ARIMA(2,0,1)(0,1,1)[12]

# 이경우의 backshift notation은...
(1-phi{1}*B)*(1-phi{2}*B[2])*(1-B[12])*y{t} = (1+theta{1}*B)*(1+THETA{1}*B[12])*e{t}
```

5. Use R to simulate and plot some data from simple ARIMA models.

- Generate data from an AR(1) model with phi_ϕ1=0.6 and σ2=1. Start with y0=0.
```{r 5_1}
# ARIMA모델에서 파이와 분산을 각각 다 지정할 수 있는 모양이다. 그걸 배우라고 낸 문제같다.
plot(a10)
la10 <- BoxCox(a10, BoxCox.lambda(a10))
length(la10)
a10Fit <- Arima(a10, order=c(1,0,0), lambda=BoxCox.lambda(a10))
a10Fit # 근데 phi1이 0.95다. 문제의 답은 아니다.

# 인터넷을 뒤져보니 arima.sim이라는 함수에서 list로 AR과 MA의 구성요소를 통제하는 것이 가능하다. 그냥 단순히 AR(1)모델을 시뮬레이션으로 생성하는 것인데,,, 헤매다녀보라는 뜻인데,,,이것도 학습의 일부인가???

ar1Fit <- arima.sim(list(order=c(1,0,0), ar=0.6, sd=1), n=100)

ar1Fit <- arima.sim(list(order=c(1,0,0), ar=0.6, sd=1, start=0), n=100) #start는 그냥 넣어본 것인데, 돌아는 간다.
plot(ar1Fit)
ar1Fit$model # 에러로 보아서는 그냥 단순히 데이터를 생성해주는 기능을 하는 것이다. 맞다 generate...

```

@@@주의할점. 실기시험장에 만약 fpp패키지가 깔려져 있지 않다면 TTR 등의 이 교재에서 활용하는 것과 다른 패키지가 일반적으로 사용되므로 그에 대해서도 알고 있어야 함.

[그에 대해서는 이 사이트 참조](http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/src/timeseries.html)
```{r}
?arima.sim
?arima
??TTR # moving average함수를 가지고 있다. (시계열이라서 증권 투자관련 함수를 응용해서 데이터를 정리할 수 있는 것이다.)
```

Produce a time plot for the series. How does the plot change as you change phi_ϕ1?
```{r 5_2}
par(mfrow=c(2,2))
arPhi = c(0.6,0.9,0.4,0.1)
ar1Fit = vector("list",4)
for (i in 1:4) {
  ar1Fit[[i]] <- arima.sim(list(order=c(1,0,0), ar=arPhi[i], sd = 1), n=100)
  plot(ar1Fit[[i]])
}
par(mfrow=c(1,1)) #phi가 작을 수록 분산이 커지고, 트렌드 변화가 없는대신, phi가 커지면 분산이 작아지고, 트렌드가 심해진다.
```

Generate data from an MA(1) model with θ1=0.6 and σ2=1. Start with e0=0.
```{r}
ma1Fit <- arima.sim(list(order=c(0,0,1), ma=.6, sd=1), n=100) # order지정시 반드시 c()함수를 사용해야 한다. 주의 필요
plot(ma1Fit)
```

Produce a time plot for the series. How does the plot change as you change θ1?
```{r}
par(mfrow=c(2,2))
maPhi = c(0.6,0.9,0.4,0.1)
ma1Fit = vector("list",4)
for (i in 1:4) {
  ma1Fit[[i]] <- arima.sim(list(order=c(0,0,1), ma=maPhi[i], sd = 1), n=100)
  plot(ma1Fit[[i]])
}
par(mfrow=c(1,1)) #theta가 작을 수록 분산의 변화가 커지는 것같다. 뭔가 차이가 나는데 잘 설명을 못하겠다.
```

Generate data from an ARMA(1,1) model with ϕ1 = 0.6 and θ1=0.6 and σ2=1. Start with y0=0 and e0=0.
```{r}
arma11Fit <- arima.sim(list(order=c(1,0,1), ar=0.6, ma=0.6, sd=1), n=100)
plot(arma11Fit)
adf.test(arma11Fit) # 간신히 stationary
kpss.test(arma11Fit)
```

Generate data from an AR(2) model with phiϕ1=−0.8 and phiϕ2=0.3 and σ2=1. Start with y0=y−1=0.
```{r}
ar2Fit <- arima.sim(list(order=c(2,0,0), ar=c(0.3, -0.8), sd=1), n=100) # ar=c(,)에서 phi2를 먼저 놓고, 다음에phi1을 넣어줘야 순서에 맞게 들어간다. 
plot(ar2Fit)
```

Graph the latter two series and compare them.
```{r}
# 이건 어떻게 하는지 잘 모르겠음. 근데, 왠지 중요한 것은 아닌 것 같음.
par(mfrow=c(2,1))
arma11Fit <- arima.sim(list(order=c(1,0,1), ar=0.6, ma=0.6, sd=1), n=100)
plot(arma11Fit)
ar2Fit <- arima.sim(list(order=c(2,0,0), ar=c(0.3, -0.8), sd=1), n=100) # ar=c(,)에서 phi2를 먼저 놓고, 다음에phi1을 넣어줘야 순서에 맞게 들어간다. 
plot(ar2Fit)
par(mfrow=c(1,1))
```

6. Consider the number of women murdered each year (per 100,000 standard population) in the United States (data set wmurders).
```{r 6_data}
wmurders #연간데이터임
plot(wmurders)
```

- By studying appropriate graphs of the series in R, find an appropriate ARIMA(p,d,q) model for these data.

@ This analysis suggests that my rule-of-thumb could be modified a little as follows: (Box.test시의 파라미터 설정 기준임.)
For non-seasonal time series, use h=\min(10, T/5).
For seasonal time series, use h=\min(2m, T/5).

@__For a stationary time series, the ACF will drop to zero relatively quickly, while the ACF of non-stationary data decreases slowly. Also, for non-stationary data, the value of r1 is often large and positive__

```{r 6_1}
tsdisplay(wmurders)
ndiffs(wmurders)
nsdiffs(wmurders)
wmurderSt <- diff(wmurders, differences=2)
kpss.test(wmurderSt) # passed
tsdisplay(wmurderSt)

wmurderFit <- Arima(wmurders, order=c(2,2,2))
wmurderFit
residuals(wmurderFit)
Box.test(residuals(wmurderFit), lag=10, fitdf=4, type="Lj") #귀무가설이 시계열내의 잔차들이 독립적이다이다. 따라서 이를 기각하지 못하였으므로 잔차들이 독립적이다라는 이야기다. (해석이 자꾸 헷갈린다). 앞에서는 제대로 했는지 모르겠는데, 이번 해석이  맞는 것같다.
?Box.test
```

Should you include a constant in the model? Explain.
```{r 6_2}
# ARIMA모델에서 상수는 평균적인 증가 혹은 감소가 있다는 의미이다. 다만 이경우는 differencing이 2이기 때문에 상수가 0이 될 것으로보인다. 
# 그래프 상으로도 wmurder rate가 사상최저수준에 근접하기 때문에 상수 반영시 하락 트렌트가 drift로 반영될 것이 예상되는 바, 반영하징 낳는 것이 예측 오차를 줄이는 방법의 하나가 될 것으로 보인다.
```

Write this model in terms of the backshift operator.
```{r 6_3}
# ARIMA(1,2,1) 기준, seasonal 효과는 없음.
(1-phi{1}*B)*(1-B)[2]*y{t} = (1+theta{1})*e{t}
```

Fit the model using R and examine the residuals. Is the model satisfactory?
```{r 6_4}
# AR과 MA의 order를 크게본 것 같음.

#검산
wmurderAuto <- auto.arima(wmurders) #AICc가 더 작다.
Box.test(residuals(wmurderAuto), lag=10, fitdf=2, type="Lj")
```

Forecast three times ahead. Check your forecasts by hand to make sure you know how they have been calculated.
```{r 6_5}
wmurderAuto
wmurderFcast <- forecast(wmurderAuto, h=3)
wmurderFcast

# 위에 적은 backshift notation을 계속 t+1, t+2, t=3으로 바꿔 주면서 계산을 해야 한다.
# ARIMA(1,2,1) 기준, 기준모델
(1-phi{1}*B)*(1-B)[2]*y{t} = (1+theta{1})*e{t}

# phi{1} = -0.2434, theta{1} = -0.8261
# backshift notation을 데이터 기준으로 푼다. 
(1--0.2434*B)*(1-2B+B[2])*y{t} = e(t) -0.8261*e{t-1}

(1+0.2434*B)*(y{t}-2*y{t-1} + y{t-2}) = -0.8261*e{t-1}

y{t}-2*y{t-1}+y{t-2} + 0.2434*y{t-1}-2*0.2434*y{t-2}+0.2434*y{t-3} = -0.8261*e{t-1}
# 따라서,
y{t} = (2-0.2434)*y{t-1} - (1-0.4868)*y{t-2} - 0.2434*y{t-3} - 0.8261*e{t-1}

# 수치를 대입하면, 
yT = 2.429415; eT = 0.001086467
yT_1 = 2.3633364; yT_2 = 2.374305; yT_3 = 2.295520
eT_1 = -0.003407084

# 논리는 이런 식인데,,,
# 수치가 조금 틀린다. 일단 이정도에서 pass....

(yT = (2-0.2434)*yT_1 - (1-0.4868)*yT_2 - 0.2434*yT_3 - 0.8261*eT_1 ) # 검산용 yT수치가 조금 틀린다. 
(yTT = (2-0.2434)*yT - (1-0.4868)*yT_1 - 0.2434*yT_2 - 0.8261*eT)
(yTTT = (2-0.2434)*yTT - (1-0.4868)*yT - 0.2434*yT_1 - 0.8261*eTT)
```

Create a plot of the series with forecasts and prediction intervals for the next three periods shown.
```{r 6_6}
# 위에서 풀었음

```

Does auto.arima give the same model you have chosen? If not, which model do you think is better?
```{r 6_7}
# auto.arima 결과 반영
wmurderFcast1 <- forecast(wmurderAuto, h=3)
plot(wmurderFcast1)
```

Find the latest data from http://data.is/XKa24F and compare with your forecasts.
```{r 6_8}
# 최근의 데이터는 없음.
```

7. Consider the quarterly number of international tourists to Australia for the period 1999–2010. (Data set austourists.)
```{r 7_1}
austourists #12년간의 분기별 데이터
```

Describe the time plot.
```{r 7_2}
tsdisplay(austourists)
```

What can you learn from the ACF graph?
```{r 7_3}
# ACF의 첫번째 lag와 5번째 lag가 critical value를 넘고 있으며, 8,12,16등이 규칙적으로 spike하고 있다. 이는 AR(1)과 Seasonal AR(1)[4]를 이야기 하는 것으로 보인다.
```

What can you learn from the PACF graph?
```{r 7_4}
# PACF의 lag1과 4,5 lag가 critical value를 spike하고 있으며, 8lag가  거의 넘을 려고 하고 있다. MA(1)과  seasonal MA(1)이 있는 것으로 보인다.
```

Produce plots of the seasonally differenced data (1−B4)Yt. What model do these graphs suggest?
```{r 7_5}
ausSeasAdj <- diff(austourists, 4)
tsdisplay(ausSeasAdj) # 여전히 1과 4 lag가 spike하고 있다.

```

Does auto.arima give the same model that you chose? If not, which model do you think is better?
```{r 7_6}
# 위에서 잠정결론은 ARIMA(1,0,1)(1,1,1)[4]였다.
ausFit <- Arima(austourists, order=c(1,0,1), seasonal=c(1,1,1), lambda=0)
ausFit # AICc가 -가 나오는데, 이걸 더 낮다고 봐야 하는지 모르겠다.

# 자동선택 함수 적용 결과
ausAuto <- auto.arima(austourists)
ausAuto #ARIMA(1,0,0)(1,1,0)[4] with drift  AICc=189.89
```

Write the model in terms of the backshift operator, and then without using the backshift operator.
```{r 7_7}
# Backshifting backshift!!!

# with My version
# 1) back shift
# (1-0.9894*B)*(1+0.1946*B[4])*(1-B[4])*y{t} = (1-0.4818*B)*(1-0.7464*B[4])*e{t}

# 2) ordinary version
# y{t} = ...
```

8. Consider the total net generation of electricity (in billion kilowatt hours) by the U.S. electric industry (monthly for the period 1985–1996). (Data set usmelec.) In general there are two peaks per year: in mid-summer and mid-winter.
```{r 8_1}
usmelec #1973.1-2010.10월 월별 데이터
plot(usmelec) # transformation 필요예상
```

Examine the 12-month moving average of this series to see what kind of trend is involved.
```{r 8_2}
usmeMa <- ma(usmelec,12)
lines(usmeMa, col="red", lwd=2)
```

Do the data need transforming? If so, find a suitable transformation.
```{r 8_3}
usmeLambda <- BoxCox.lambda(usmelec)
usmeLambda
usmeTr <- BoxCox(usmelec, usmeLambda)
plot(usmeTr)
```

Are the data stationary? If not, find an appropriate differencing which yields stationary data.
```{r 8_4}
# 분산은 안정화되었으나, 시즌널리티, 트랜드 등이 있는 모양새임.
ndiffs(usmeTr)
nsdiffs(usmeTr) # seasonality는 없음.
usmeSt <- diff(usmeTr)
kpss.test(usmeSt) #귀무가설 기각 실패, stationary
tsdisplay(usmeSt)
```

Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values?
```{r 8_5}
# ACF가 sine곡선을 따르고 있으며, 12,24,36 주기적으로 spike가 일어나고 있음. PACF도 12lag까지 spike지속 및 12, 23, 36 발생
(usmeFit1 <- Arima(usmelec, order=c(1,1,1), seasonal=c(0,1,1)))

(usmeFit2 <- Arima(usmelec, order=c(1,1,0), seasonal=c(0,0,1)))
```

Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.
```{r 8_6}
Box.test(residuals(usmeFit1), lag=10, fitdf=3) #간신히 귀무가설 기각 실패. whitenoise임. forecasting가능

Box.test(residuals(usmeFit2), lag=10, fitdf=2) #귀무가설 기각. not whitenoise. forecasting불가
```

Forecast the next 15 years of generation of electricity by the U.S. electric industry. Get the latest figures from http://data.is/zgRWCO to check on the accuracy of your forecasts.
```{r 8_7}
usmeFcast <- forecast(usmeFit1, h=15*12)
usmeFcast
plot(usmeFcast)
resultdiff <- matrix(c(363.105, 354.7981,
                       337.961, 341.8184,
                       335.753, 346.3878,
                       337.530, 327.0480,
                       396.108, 401.3065,
                       325.372, 320.4043,
                       356.400, 367.7854), byrow=TRUE, nrow=7, ncol=2)
matplot(resultdiff, type="b") #2번이 예측치, 1번이 실제 수치
```

How many years of forecasts do you think are sufficiently accurate to be usable?
```{r 8_8}
# 큰 추세의 변화가 없다면 계속 사용해도 좋을 것 같음.
```

9. For the mcopper data:
```{r 9_1}
mcopper #월별 데이터임.
plot(mcopper)
```

if necessary, find a suitable Box-Cox transformation for the data;
```{r 9_2}
(lambda <- BoxCox.lambda(mcopper))
mcopperTr <- BoxCox(mcopper, lambda)
tsdisplay(mcopperTr)
```

fit a suitable ARIMA model to the transformed data using auto.arima();
```{r 9_3}
mcopperFit <- auto.arima(mcopperTr)
mcopperFit
Box.test(residuals(mcopperFit)) #귀무가설 기각실패, whitenoise
```

try some other plausible models by experimenting with the orders chosen;
```{r 9_4}
(mcopperFit2 <- Arima(mcopperTr, order=c(1,1,1)))

(mcopperFit3 <- Arima(mcopperTr, order=c(0,1,2)))

(mcopperFit4 <- Arima(mcopperTr, order=c(0,2,1)))
```

choose what you think is the best model and check the residual diagnostics;
```{r 9_5}
mcopperFit
Box.test(residuals(mcopperFit)) #귀무가설 기각실패, whitenoise
```

produce forecasts of your fitted model. Do the forecasts look reasonable?
```{r 9_6}
mcopperFcast <- forecast(mcopperFit, h=3*12)
plot(mcopperFcast) # randomwalk와 같은 결론임. drift도 없음.
```

compare the results with what you would obtain using ets() (with no transformation).
```{r 9_7}
mcopperFit5 <- ets(mcopper)
mcopperFit5
mcopperEtsFcast <- forecast(mcopperFit5, h=36)
plot(mcopperEtsFcast)
```

10. Choose one of the following seasonal time series: condmilk, hsales, uselec
```{r 10_1}
plot(condmilk)
plot(hsales) # 이걸로 함.
plot(uselec) 
```

Do the data need transforming? If so, find a suitable transformation.
```{r 10_2}
hsales #월별 데이터
tsdisplay(hsales)
lambda <- BoxCox.lambda(hsales)
hsaleTr <- BoxCox(hsales, lambda)
tsdisplay(hsaleTr)
```

Are the data stationary? If not, find an appropriate differencing which yields stationary data.
```{r 10_3}
adf.test(hsaleTr) # stationary
kpss.test(hsaleTr)
```

Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AIC values?
```{r 10_4}
myFit <- Arima(hsaleTr, order=c(1,1,0), seasonal=c(0,1,1))
myFit
autoFit <- auto.arima(hsaleTr)
autoFit #AIC가 조금더 낮음.
```

Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.
```{r 10_5}
Box.test(residuals(myFit), lag=10, fitdf=2) #귀무가설 기각 실패. Whitenoise임.
Box.test(residuals(autoFit), lag=10, fitdf=6) #귀무가설 기각 실패. Whitenoise임.
```

Forecast the next 24 months of data using your preferred model.
Compare the forecasts obtained using ets().
```{r 10_6}
par(mfrow=c(1,1))
myFcast <- forecast(myFit, h=24)
plot(myFcast)

autoFcast <- forecast(autoFit, h=24)
plot(autoFcast) #이쪽이 예측오차 범위가 좁음.
```

11. For the same time series you used in exercise Q10, try using a non-seasonal model applied to the seasonally adjusted data obtained from STL. The stlf() function will make the calculations easy (with method="arima"). Compare the forecasts with those obtained in exercise Q10. Which do you think is the best approach?
```{r 11_1}
hsaleStl <- stlf(hsales, method="arima")
plot(hsaleStl)
hsaleStl$model  # AIC가 Arima모델이 더 좋음.
```