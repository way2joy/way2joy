이번 주에 계속 전북대학교 컴퓨터 공학과 오일석 교수님이 집필하신 “패턴 인식”이라는 책을 읽고 있다.



지난 약 8개월간 MOOC나 인터넷 등을 통해 공부를 하다가, 누군가의 소개로 이책을 알게 되었는데, 흡사 춘향이가 이도령 만난 듯하고, 독일 볼프스부르크에 한달간 출장가 박혀 있는 와중에, 연락도 없이 와이프가 따뜻한 쌀밥에 잘 익은  김치를 들고 찾아온 듯한 기분이 든다.



지금 데이터 모델링 분야가 붐을 타고, 각사의 마케팅을 위한 목적으로 시끄럽기는 하지만, 컴퓨터 공학 쪽에서는 아주 오래전부터 인간이 아닌 컴퓨터가 인식 능력을 가지고 행동하는 듯하게 만들기 위해 이러한 패턴인식에 대한 연구를 하고 있었고,



이러한 연구 성과들을 데이터 마이닝이나 통계관련 분야에서 거의 그대로 가져다 쓰고 있는 형편이어서, 쓰는 용어들이나 다루는 주제들이 크게 차이가 나지는 않지만,



나와 같은 Self-study 초보자들에게는 오히려 알고리즘을 따지는 이러한 공학적 설명이 더 명확하게 이해를 할 수 있게 도와주는 것 같다.



데이터 분석관련 분야를 self-study로 공부할려면 5중고를 겪어야 한다.



일단 맨땅에 헤딩을 피할려면 MOOC등을 활용해야 되니 영어가 되야하고, 영어가 되어도 통계학이나 데이터 마이닝 관련된 terminology는 따로 습득해야 되며, 어떻게 이것들을 안다고 하더라도 이들을 공부할 때 사용되는 R과 같은 툴 들은 물론 Python, Java같은 language를 또한 같이 알아야 하고, 4번째로는 이들을 효과적으로 실무적으로 활용 하기위해서는 Linux나 웹 플랫폼에서 사용하는 기술들이나 이들을 사용하는 방법을 알아야 한다. 그리고 마지막으로는 이 모든 것을 한번에 배울 수 없으니, 상당시간 동안을 가능한한 시간의 낭비 없이 지속적으로 “나는 왜 이렇게 모르는 것만 있나”하는 기분을 참아가며 노력을 이어 나가야 한다.



근데 이 책은 일단 한글이다. 이 얼마나 좋은가? 게다가 설명 방식도 학생들을 많이 가르쳐 본 경험으로 가장 납득하기 좋은 방법을 알고 계시는 데다가 수학도 적절한 수준으로 활용할려고 노력까지 해주신다.  하지만 그렇다고 내용이 부족하거나 다루는 범위가 한정되어 있지도 않은 것 같다. (물론 그렇다고 척 보면 알수 있는 수준의 수학은 아니다. 그나마 낫다는 의미임을 이해해 주시기 바란다.)



그리고 스승으로서의 자세를 잘 견지하고 계신다. 사실 나는 이책의 머릿말을 보고서, 끝까지 간다라는 생각을 했었다. “때로 한글은 과학기술을 서술하는데 부족함이 많다고 말하는 사람을 만나곤 한다. 그들의 생각은 틀렸다.한글의 부족함 때문이 아니라 개인의 표현력 미숙때문이다.” 멋지지 않은가?



실제로 읽어보면  상당히 공을 들여 번역을 하셔서 거의 거부감이 없는 정도다.  특히 데이터 분석을 하다가 보니 어떤  특징을 추출하여 집단내의 소그룹 등을 찾거나 차이를 비교해 특정 집단으로 분류하는 일들이 많은데, 이들 각각을 영어로는 통상 “class”라고 하는데, 이것을 “부류”로 번역하신 것이 최고의 압권이다.



그리고 어려운 개념을 설명하고 나면, 예외를 두지않고 꼭 손으로 풀 수 있는 예제들을 두어서 개념을 명확하게 파악하게 만들어 주신다. 선형대수란 것이 2차원에서 설명되면 1000차원이나 만차원, 심지억 몇억 차원도 같은 논리로 해결할 수 있기에 이런 과정은 꼭 필요한데,  실제로 그렇게 하는 책은 많지가 않은 걸 보면 쉽지않은 일인 것이 분명하다.



다만, 컴퓨터 공학쪽에서는 통계학에서 중요하게 생각하는 표본 통계량과 모집단 통계량의 차이 구분을 하지 않으시는 모양이다. 그래서 책에 나오는 계산을 통계 패키지로 구현을 해보면 숫자의 차이가 난다. 왜 그런가 추적을 해 보았더니, 통계학 쪽에서는 샘플의 수가 적을 경우, 불편 추정량을 계산하기 위해서 분모를 (n-1)로해서 나누는데,



컴퓨터 공학적인 관점에서는 갸갸 갼데 뭘 그래? 하시는지, 원래의 분산(variance) 계산 공식 그대로 (n)으로 나누니 약간씩의 차이가 나는 것 같다.



하지만 이런 사소한 부분 말고는 연습문제가 자유분방해서 어렵다는 점 하나와 한번 읽어서는 잘 모르겠다는 것 외에 다른 점은 크게 불만이 없다.



원칙은 도서관에서 빌려서 보는 것으로 끝내는 것이 이 블로그에서 주장하는 Self-study의 입장인데, 이 책은 예외로 할 수 밖에 없을 것 같다.



방금 교보문고에 주문했다. 근데, 너무너무 기대된다.



이제 막 낙서하면서 공부하고,  문제도 풀어보고 할  수 있을 테니 말이다. 하하하.



x <- matrix(c(2,1,2,4,4,1,4,3), byrow=T, ncol=2)

x

cov(x)

cov(t(x))

var(x)



y <- matrix(c(2,1,2,4,4,1,4,3), byrow=F, nrow=2)

y

meanY <- rowMeans(y)



#샘플 cov인지, 모집단 cov로 보는지에 따라 수치가 조금씩 달라질 수 있다.

rowSums(((y - meanY)^2))/3 



cov(t(y))



projectY <- t(eigen(cov(t(y)))$vectors[,1]) %*% y

mean(projectY)

sum((projectY - mean(projectY))^2)/4

sum((projectY - mean(projectY))^2)/3

var(projectY[1,])



x <- matrix(c(2,1,2,4,4,1,4,3), byrow=T, ncol=2)

x

cov(x)

cov(t(x))

var(x)

 

y <- matrix(c(2,1,2,4,4,1,4,3), byrow=F, nrow=2)

y

meanY <- rowMeans(y)

 

#샘플 cov인지, 모집단 cov로 보는지에 따라 수치가 조금씩 달라질 수 있다.

rowSums(((y - meanY)^2))/3 

 

cov(t(y))

 

projectY <- t(eigen(cov(t(y)))$vectors[,1]) %*% y

mean(projectY)

sum((projectY - mean(projectY))^2)/4

sum((projectY - mean(projectY))^2)/3

var(projectY[1,])