지난 주 부터 정식으로 수업을 시작한 caltech의 “learning from data” course를 소개해 드립니다. 저는 지난 5월에 Googling을 하다가 우연히 녹화된 동영상을 2편인가 보았는데, 내용은 좋은 것 같은데, edx에서 정식으로 제공되는 discussion forum이 없는데다가, 배운 정도를 확인할 수 있는 문제 등이 없어서, 혼자 공부하기는 무리라 그동안 잊고 있었는데,



지지난주에 edx의 10월개강 수업소개 메일이 와서 보다가 수업의 시작을 알고 수강하게 되었습니다.



저자가 이야기 하는 바, Caltech의 수업의 수준을 그대로 옮겼다고 하는 강의로,

흔히 MOOC에서는 수업을 듣기 편하도록 10~15분으로 나누어서 편성을 하는데 비해 이것은 대학교의 3학점 강의 처럼, 1시간 20분정도의 강의를 1주에 2회 강의를 합니다.



5월 당시에는 몰랐는데, 정식으로 강의를 들으니, 제일 마지막 강의를 맛보기 강의로 보여주더군요. 특이하다 싶었는데, 왜 이런 강의를 하게 되었는지를 설명하는 부분이 있었습니다.



원래부터 저작권 없이 온라인으로 공개를 목적으로 제작된 강의입니다. 교수님이 용의주도하게 관련 후원자들을 모집하고, 영원히 남겨서 누가보더라도 흠잡을데 없는 강의를 만들려고 기획했다는 것을 알 수 있었습니다.



1주차 과정은 제가 지난 번에 한번 들어본 내용들인데도, 새롭고, 그간의 학습으로 인해

내용도 좀 더 마음에 와닿았습니다.



하지만, 언제나 느끼는 것이지만, MOOC의 백미는 discussion forum과 문제 풀이입니다.

문제를 보면, 교수님들이 학생들에게 어느 정도의 수준을 기대하는 지를 알 수 있습니다.

그리고, MOOC에서는 희귀하게도 이 수업은 수업이후에 Q&A까지도 전부 화면으로 제공을 합니다. 교수님의 태도는 무엇이든 물어보라, 다 답해 주겠다는 자신감을 보이고 있고, 겨우 처음 2편만 보았을 뿐입니다만,



물어 보는 학생들을 질문을 들어보면, “선생님들이 저런 학생들을 가르키는 것이 얼마나 행복할까?”하는 생각이 듭니다.



다만, 문제는 그에 맞게 만만치 않습니다. 하지만 저희에게는 discussion forum이 있으니,

다행이지요. 베이즈 정리를 가르켜 주지도 않고, 베이즈 정리 문제를 내고, 어떻게 프로그래밍하라고  가르쳐 주지 않고, 프로그래밍을 해야 풀 수 있는 문제를 냅니다.



(물론 원리는 수업중에 스치 듯 가르쳐 주긴 했습니다. 그러니 얄미워도 어쩔 수는 없습니다. ㅠㅠ)



3일 연휴의 하루를 몽땅 할애했습니다만, 행복했습니다.

쉽지 않은 강좌이오니, 지금 듣기 어려우신 분들은 등록해서 문제라도 받아놓으시면, 수업은 서버가 망가지는 날까지 온라인으로 제공되니, 나중에라도 공부하실 수 있으실 것 같네요.





perceptron learning algorithm 설명자료

 아래 perceptron 프로그램은 제가 이곳저곳 보고 만들기는 했는데, 정답은 아닙니다. 그냥 돌아가는 수준이라 첨부합니다. 정답이면 올리면 안되겠지요.



#PLA algorithm



numOfTests <- 1000

finalIteration <- rep(0,numOfTests)

testResult <- perceptron_caltech(10,numOfTests)

mean(testResult)





numOfTests <- 1000

finalIteration <- rep(0,numOfTests)

testResult <- perceptron_caltech(100,numOfTests)

mean(testResult)





perceptron_caltech <- function(numSample, numOfTests) {

  for( num in 1:numOfTests) {

        generated = data.generate(n= numSample, ext=1)

        generated

        independent = as.matrix(cbind(1, generated$data[c(1,2)]))

        independent

        

        w = c(0,0,0)

        numExamples <- nrow(independent)

        numExamples

        results <- rep(0,nrow(independent))

        iteration <- 0

        positives <- which(generated$data$y == 1)

        length(positives)

        positives

        negatives <- which(generated$data$y != 1)

        length(negatives)

        negatives

        

        for(i in 1:numExamples) {

            x <- independent[i,]

            activation <- x %*% w

            if(activation >= 0) {

              results[i] <- 1

            } else {

              results[i] <- -1

            }

        }

        (error <- sum(results != generated$data$y))

        

        while(error != 0) {

            iteration <- iteration + 1

            for(i in negatives) {

                  this_case <- independent[i,]

                  activation <- this_case %*% w

                  if(activation >= 0) {

                       w <- w - this_case

                  }

             }

         

             for(i in positives) {

                  this_case <- independent[i,]

                  activation <- this_case %*% w

                  if(activation < 0) {

                       w <- w + this_case  

                  }

             } 

        

           for(i in 1:numExamples) {

                x <- independent[i,]

                activation <- x %*% w

                if(activation >= 0) {

                  results[i] <- 1

                } else {

                  results[i] <- -1

                }

            }

            error <- sum(results != generated$data$y)

        }

        

  finalIteration[num] <- iteration      

  }

  return(finalIteration)

}



w

results

positives

negatives



# sample data generation_from_chaoping's posting on discussion forum



data.generate = function(n = 10, ext = 1){ 

  # Generate the points.

  x1 = runif(n, -ext, ext) # random uniform distribution

  x2 = runif(n, -ext, ext)

	

	# Draw a random line in the area.

	point = runif(2, -ext, ext)

	point2 = runif(2, -ext, ext)

	slope = (point2[2] - point[2]) / (point2[1] - point[1])

	intercept = point[2] - slope * point[1]

	

	# Assign the dependent values.

	y = as.numeric(x1 * slope + intercept > x2) * 2 - 1

	

	# Return the values.

	data = data.frame(x1,x2,y)

	return(list(data = data,slope = slope, intercept = intercept))

}	



data.generate()



generated = data.generate(n= 10, ext=1)

generated



qplot(x1,x2,col= as.factor(y), data = generated$data) + geom_abline(intercept = generated$intercept, slope = generated$slope)



#PLA algorithm

 

numOfTests <- 1000

finalIteration <- rep(0,numOfTests)

testResult <- perceptron_caltech(10,numOfTests)

mean(testResult)

 

 

numOfTests <- 1000

finalIteration <- rep(0,numOfTests)

testResult <- perceptron_caltech(100,numOfTests)

mean(testResult)

 

 

perceptron_caltech <- function(numSample, numOfTests) {

  for( num in 1:numOfTests) {

        generated = data.generate(n= numSample, ext=1)

        generated

        independent = as.matrix(cbind(1, generated$data[c(1,2)]))

        independent

        

        w = c(0,0,0)

        numExamples <- nrow(independent)

        numExamples

        results <- rep(0,nrow(independent))

        iteration <- 0

        positives <- which(generated$data$y == 1)

        length(positives)

        positives

        negatives <- which(generated$data$y != 1)

        length(negatives)

        negatives

        

        for(i in 1:numExamples) {

            x <- independent[i,]

            activation <- x %*% w

            if(activation >= 0) {

              results[i] <- 1

            } else {

              results[i] <- -1

            }

        }

        (error <- sum(results != generated$data$y))

        

        while(error != 0) {

            iteration <- iteration + 1

            for(i in negatives) {

                  this_case <- independent[i,]

                  activation <- this_case %*% w

                  if(activation >= 0) {

                       w <- w - this_case

                  }

             }

         

             for(i in positives) {

                  this_case <- independent[i,]

                  activation <- this_case %*% w

                  if(activation < 0) {

                       w <- w + this_case  

                  }

             } 

        

           for(i in 1:numExamples) {

                x <- independent[i,]

                activation <- x %*% w

                if(activation >= 0) {

                  results[i] <- 1

                } else {

                  results[i] <- -1

                }

            }

            error <- sum(results != generated$data$y)

        }

        

  finalIteration[num] <- iteration      

  }

  return(finalIteration)

}

 

w

results

positives

negatives

 

# sample data generation_from_chaoping's posting on discussion forum

 

data.generate = function(n = 10, ext = 1){ 

  # Generate the points.

  x1 = runif(n, -ext, ext) # random uniform distribution

  x2 = runif(n, -ext, ext)

    

    # Draw a random line in the area.

    point = runif(2, -ext, ext)

    point2 = runif(2, -ext, ext)

    slope = (point2[2] - point[2]) / (point2[1] - point[1])

    intercept = point[2] - slope * point[1]

    

    # Assign the dependent values.

    y = as.numeric(x1 * slope + intercept > x2) * 2 - 1

    

    # Return the values.

    data = data.frame(x1,x2,y)

    return(list(data = data,slope = slope, intercept = intercept))

}    

 

data.generate()

 

generated = data.generate(n= 10, ext=1)

generated

 

qplot(x1,x2,col= as.factor(y), data = generated$data) + geom_abline(intercept = generated$intercept, slope = generated$slope)

그리고, 마지막으로 아끼는 후배가 소개해준 영화 begin again “OST”입니다. 덕분에 마이 행복했습니다.  혹시 아직 못들어보신 분들은 한번 즐겨보시기 바랍니다.